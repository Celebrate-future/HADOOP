<workflow-app xmlns="uri:oozie:workflow:0.4" name="sages-oozie">

    <start to="hive"/>

    <action name="hive">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${hiveOutputPath}"/>
            </prepare>
            <script>hive-script.hql</script>
            <param>OUTPUT=${hiveOutputPath}</param>
        </hive>
        <ok to="sqoopa"/>
        <error to="fail"/>
    </action>

    <action name="sqoopa">
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${sqoopOutputPath}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.compress.map.output</name>
                    <value>true</value>
                </property>
            </configuration>
            <command>import --connect jdbc:mysql://localhost:3306/sakila --table customer --username sages --password
                password --driver com.mysql.jdbc.Driver --warehouse-dir /user/sages/sqoop/ --num-mappers 1
            </command>
        </sqoop>
        <ok to="sqoopb"/>
        <error to="fail"/>
    </action>

    <action name="sqoopb">
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${sqoopOutputPath}"/>
            </prepare>
            <configuration>
                <property>
                    <name>mapred.compress.map.output</name>
                    <value>true</value>
                </property>
            </configuration>
            <arg>import</arg>
            <arg>--connect</arg>
            <arg>jdbc:mysql://localhost:3306/sakila</arg>
            <arg>--table</arg>
            <arg>customer</arg>
            <arg>--username</arg>
            <arg>sages</arg>
            <arg>--password</arg>
            <arg>password</arg>
            <arg>--driver</arg>
            <arg>com.mysql.jdbc.Driver</arg>
            <arg>--warehouse-dir</arg>
            <arg>/user/sages/sqoop/</arg>
            <arg>--num-mappers</arg>
            <arg>1</arg>
        </sqoop>
        <ok to="distcp"/>
        <error to="fail"/>
    </action>

    <action name="distcp">
        <distcp xmlns="uri:oozie:distcp-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <arg>${distcpInputPath}</arg>
            <arg>${distcpOutputPath}</arg>
        </distcp>
        <ok to="wordcount"/>
        <error to="fail"/>
    </action>

    <action name="wordcount">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${nameNode}${wordcountOutputPath}"/>
            </prepare>
            <configuration>

                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>

                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>

                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>pl.com.sages.hadoop.mapreduce.wordcount.WordCountMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>pl.com.sages.hadoop.mapreduce.wordcount.WordCountReducer</value>
                </property>

                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.IntWritable</value>
                </property>

                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${wordcountInputPath}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${wordcountOutputPath}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="invertedindex"/>
        <error to="fail"/>
    </action>

    <action name="invertedindex">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${nameNode}${invertedindexOutputPath}"/>
            </prepare>
            <configuration>

                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>

                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>

                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>pl.com.sages.hadoop.mapreduce.invertedindex.InvertedIndexMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>pl.com.sages.hadoop.mapreduce.invertedindex.InvertedIndexReducer</value>
                </property>

                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>

                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${invertedindexInputPath}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${invertedindexOutputPath}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="end"/>
        <error to="fail"/>
    </action>

    <kill name="fail">
        <message>Map/Reduce failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>

    <end name="end"/>

</workflow-app>
